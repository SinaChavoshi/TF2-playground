{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import logging\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as kl\n",
    "import tensorflow.keras.losses as kls\n",
    "import tensorflow.keras.optimizers as ko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Ver:  2.0.0-dev20190426\n",
      "Eager Execution: True\n"
     ]
    }
   ],
   "source": [
    "print(\"TensorFlow Ver: \", tf.__version__)\n",
    "print(\"Eager Execution:\", tf.executing_eagerly())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 + 2 + 3 + 4 + 5 = tf.Tensor(15, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# eager by default!\n",
    "print(\"1 + 2 + 3 + 4 + 5 =\", tf.reduce_sum([1, 2, 3, 4, 5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advantage Actor-Critic with TensorFlow 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy & Value Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProbabilityDistribution(tf.keras.Model):\n",
    "    def call(self, logits):\n",
    "        # sample a random categorical action from given logits\n",
    "        return tf.squeeze(tf.random.categorical(logits, 1), axis=-1)\n",
    "\n",
    "class Model(tf.keras.Model):\n",
    "    def __init__(self, num_actions):\n",
    "        super().__init__('mlp_policy')\n",
    "        # no tf.get_variable(), just simple Keras API\n",
    "        self.hidden1 = kl.Dense(128, activation='relu')\n",
    "        self.hidden2 = kl.Dense(128, activation='relu')\n",
    "        self.value = kl.Dense(1, name='value')\n",
    "        # logits are unnormalized log probabilities\n",
    "        self.logits = kl.Dense(num_actions, name='policy_logits')\n",
    "        self.dist = ProbabilityDistribution()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # inputs is a numpy array, convert to Tensor\n",
    "        x = tf.convert_to_tensor(inputs)\n",
    "        # separate hidden layers from the same input tensor\n",
    "        hidden_logs = self.hidden1(x)\n",
    "        hidden_vals = self.hidden2(x)\n",
    "        return self.logits(hidden_logs), self.value(hidden_vals)\n",
    "\n",
    "    def action_value(self, obs):\n",
    "        # executes call() under the hood\n",
    "        logits, value = self.predict(obs)\n",
    "        action = self.dist.predict(logits)\n",
    "        # a simpler option, will become clear later why we don't use it\n",
    "        # action = tf.random.categorical(logits, 1)\n",
    "        return np.squeeze(action, axis=-1), np.squeeze(value, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advantage Actor-Critic Agent Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A2CAgent:\n",
    "    def __init__(self, model):\n",
    "        # hyperparameters for loss terms, gamma is the discount coefficient\n",
    "        self.params = {\n",
    "            'gamma': 0.99,\n",
    "            'value': 0.5,\n",
    "            'entropy': 0.0001\n",
    "        }\n",
    "        self.model = model\n",
    "        self.model.compile(\n",
    "            optimizer=ko.RMSprop(lr=0.0007),\n",
    "            # define separate losses for policy logits and value estimate\n",
    "            loss=[self._logits_loss, self._value_loss]\n",
    "        )\n",
    "    \n",
    "    def train(self, env, batch_sz=32, updates=1000):\n",
    "        # storage helpers for a single batch of data\n",
    "        actions = np.empty((batch_sz,), dtype=np.int32)\n",
    "        rewards, dones, values = np.empty((3, batch_sz))\n",
    "        observations = np.empty((batch_sz,) + env.observation_space.shape)\n",
    "        # training loop: collect samples, send to optimizer, repeat updates times\n",
    "        ep_rews = [0.0]\n",
    "        next_obs = env.reset()\n",
    "        for update in range(updates):\n",
    "            for step in range(batch_sz):\n",
    "                observations[step] = next_obs.copy()\n",
    "                actions[step], values[step] = self.model.action_value(next_obs[None, :])\n",
    "                next_obs, rewards[step], dones[step], _ = env.step(actions[step])\n",
    "\n",
    "                ep_rews[-1] += rewards[step]\n",
    "                if dones[step]:\n",
    "                    ep_rews.append(0.0)\n",
    "                    next_obs = env.reset()\n",
    "                    logging.info(\"Episode: %03d, Reward: %03d\" % (len(ep_rews)-1, ep_rews[-2]))\n",
    "\n",
    "            _, next_value = self.model.action_value(next_obs[None, :])\n",
    "            returns, advs = self._returns_advantages(rewards, dones, values, next_value)\n",
    "            # a trick to input actions and advantages through same API\n",
    "            acts_and_advs = np.concatenate([actions[:, None], advs[:, None]], axis=-1)\n",
    "            # performs a full training step on the collected batch\n",
    "            # note: no need to mess around with gradients, Keras API handles it\n",
    "            losses = self.model.train_on_batch(observations, [acts_and_advs, returns])\n",
    "            logging.debug(\"[%d/%d] Losses: %s\" % (update+1, updates, losses))\n",
    "        return ep_rews\n",
    "\n",
    "    def test(self, env, render=False):\n",
    "        obs, done, ep_reward = env.reset(), False, 0\n",
    "        while not done:\n",
    "            action, _ = self.model.action_value(obs[None, :])\n",
    "            obs, reward, done, _ = env.step(action)\n",
    "            ep_reward += reward\n",
    "            if render:\n",
    "                env.render()\n",
    "        return ep_reward\n",
    "\n",
    "    def _returns_advantages(self, rewards, dones, values, next_value):\n",
    "        # next_value is the bootstrap value estimate of a future state (the critic)\n",
    "        returns = np.append(np.zeros_like(rewards), next_value, axis=-1)\n",
    "        # returns are calculated as discounted sum of future rewards\n",
    "        for t in reversed(range(rewards.shape[0])):\n",
    "            returns[t] = rewards[t] + self.params['gamma'] * returns[t+1] * (1-dones[t])\n",
    "        returns = returns[:-1]\n",
    "        # advantages are returns - baseline, value estimates in our case\n",
    "        advantages = returns - values\n",
    "        return returns, advantages\n",
    "    \n",
    "    def _value_loss(self, returns, value):\n",
    "        # value loss is typically MSE between value estimates and returns\n",
    "        return self.params['value']*kls.mean_squared_error(returns, value)\n",
    "\n",
    "    def _logits_loss(self, acts_and_advs, logits):\n",
    "        # a trick to input actions and advantages through same API\n",
    "        actions, advantages = tf.split(acts_and_advs, 2, axis=-1)\n",
    "        # sparse categorical CE loss obj that supports sample_weight arg on call()\n",
    "        # from_logits argument ensures transformation into normalized probabilities\n",
    "        weighted_sparse_ce = kls.SparseCategoricalCrossentropy(from_logits=True)\n",
    "        # policy loss is defined by policy gradients, weighted by advantages\n",
    "        # note: we only calculate the loss on the actions we've actually taken\n",
    "        actions = tf.cast(actions, tf.int32)\n",
    "        policy_loss = weighted_sparse_ce(actions, logits, sample_weight=advantages)\n",
    "        # entropy loss can be calculated via CE over itself\n",
    "        entropy_loss = kls.categorical_crossentropy(logits, logits, from_logits=True)\n",
    "        # here signs are flipped because optimizer minimizes\n",
    "        return policy_loss - self.params['entropy']*entropy_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'gym' has no attribute 'make'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-239680fffafb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'CartPole-v0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_actions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'gym' has no attribute 'make'"
     ]
    }
   ],
   "source": [
    "env = gym\n",
    ".make('CartPole-v0')\n",
    "model = Model(num_actions=env.action_space.n)\n",
    "model.action_value(env.reset()[None, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training A2C Agent & Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "model = Model(num_actions=env.action_space.n)\n",
    "agent = A2CAgent(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing with Random Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_sum = agent.test(env)\n",
    "print(\"Total Episode Reward: %d out of 200\" % agent.test(env))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set to logging.WARNING to disable logs or logging.DEBUG to see losses as well\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "rewards_history = agent.train(env)\n",
    "print(\"Finished training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing with Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total Episode Reward: %d out of 200\" % agent.test(env))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Rewards History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn')\n",
    "plt.plot(np.arange(0, len(rewards_history), 25), rewards_history[::25])\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Static Computational Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Graph().as_default():\n",
    "    print(\"Eager Execution:\", tf.executing_eagerly()) # False\n",
    "\n",
    "    model = Model(num_actions=env.action_space.n)\n",
    "    agent = A2CAgent(model)\n",
    "\n",
    "    rewards_history = agent.train(env)\n",
    "    print(\"Finished training, testing...\")\n",
    "    print(\"Total Episode Reward: %d out of 200\" % agent.test(env))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "obs = np.repeat(env.reset()[None, :], 100000, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: wall time doesn't show the whole picture, better to compare CPU time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eager Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model = Model(env.action_space.n)\n",
    "model.run_eagerly = True\n",
    "\n",
    "print(\"Eager Execution:  \", tf.executing_eagerly())\n",
    "print(\"Eager Keras Model:\", model.run_eagerly)\n",
    "\n",
    "_ = model(obs)\n",
    "# _ = model.predict(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Static Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    model = Model(env.action_space.n)\n",
    "\n",
    "    print(\"Eager Execution:  \", tf.executing_eagerly())\n",
    "    print(\"Eager Keras Model:\", model.run_eagerly)\n",
    "\n",
    "    _ = model.predict(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model = Model(env.action_space.n)\n",
    "\n",
    "print(\"Eager Execution:  \", tf.executing_eagerly())\n",
    "print(\"Eager Keras Model:\", model.run_eagerly)\n",
    "\n",
    "_ = model.predict(obs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
